{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee52ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiy.vision.inference import CameraInference\n",
    "from aiy.vision.models import face_detection\n",
    "from aiy.vision.streaming.server import StreamingServer\n",
    "from aiy.vision.streaming import svg\n",
    "from aiy.leds import Leds, Color\n",
    "from gpiozero import Servo\n",
    "from aiy.pins import PIN_A, PIN_B\n",
    "\n",
    "from picamera import PiCamera\n",
    "from IPython.display import Image, display, clear_output\n",
    "\n",
    "import contextlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269c4f0",
   "metadata": {},
   "source": [
    "### Robot facing face\n",
    "\n",
    "We are going to use the servo motor to point at the face that was detected. We will use the bounding box to determine the angle. And remember if your joy detector is running, you need to turn it off using the commands\n",
    "\n",
    "```\n",
    "sudo systemctl stop joy_detection_demo.service\n",
    "```\n",
    "\n",
    "First we  load our robot extenstion. THis is a version running a separate thread so the wait does not stop the face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5f4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot import RobotService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc329e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/gpiozero/output_devices.py:1533: PWMSoftwareFallback: To reduce servo jitter, use the pigpio pin factory.See https://gpiozero.readthedocs.io/en/stable/api_output.html#servo for more info\n",
      "  'To reduce servo jitter, use the pigpio pin factory.'\n"
     ]
    }
   ],
   "source": [
    "bot=RobotService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67328dc",
   "metadata": {},
   "source": [
    "I added some code here that is called to create the overlay - basically the box around the face and the score above. You can customize it and/or add information you want to overlay on the camera feed. **This is not needed **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a383051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_overlay(faces, frame_size):\n",
    "\n",
    "    JOY_COLOR = (255, 70, 0)\n",
    "    SAD_COLOR = (0, 0, 64)\n",
    "\n",
    "    width, height = frame_size\n",
    "    doc = svg.Svg(width=width, height=height)\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.bounding_box\n",
    "        fcol='rgb'+str(Color.blend(JOY_COLOR, SAD_COLOR, face.joy_score))\n",
    "        doc.add(svg.Rect(x=int(x), y=int(y), width=int(w), height=int(h), rx=10, ry=10,\n",
    "                         fill_opacity=0.3 * face.face_score,\n",
    "                         style='fill:'+fcol+';stroke:white;stroke-width:4px'))\n",
    "\n",
    "        doc.add(svg.Text('Joy: %.2f' % face.joy_score, x=x, y=y-10, fill='red', font_size=30))\n",
    "\n",
    "    return str(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de67ca5",
   "metadata": {},
   "source": [
    "#### Main loop\n",
    "\n",
    "Here is our main loop based on the code we used last time. Look at the comments on what was changed. Once we see a face, we call the routine `pointFace` which returns the offset of the face boudary box from the center. We also added the streaming back in, to while this cell runs, you can connect to http://orcspi-vis.local:4664 and see the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032279c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointFace(box, width):\n",
    "    \n",
    "    x, y, w, h = box\n",
    "    middle = x + w/2\n",
    "    offset = 2*middle/width - 1\n",
    "\n",
    "    print (\"Offset: {offset:.2f}, Middle: {middle}\".format(offset=offset, middle=middle))\n",
    "    \n",
    "    return -offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06726e24",
   "metadata": {},
   "source": [
    "And the main loop we already know so well :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f89e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset: -0.04, Middle: 790.5\n",
      "Interrupted ..\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[192.168.0.155:53627] Error while processing websocket request\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pi/AIY-projects-python/src/aiy/vision/streaming/server.py\", line 570, in _receive_message\n",
      "    packet = self._receive_packet()\n",
      "  File \"/home/pi/AIY-projects-python/src/aiy/vision/streaming/server.py\", line 612, in _receive_packet\n",
      "    packet.fin = buf[0] & 0x80 > 0\n",
      "IndexError: index out of range\n"
     ]
    }
   ],
   "source": [
    "with contextlib.ExitStack() as stack:\n",
    "    leds   = stack.enter_context(Leds())\n",
    "    camera = stack.enter_context(PiCamera(sensor_mode=4, resolution=(820, 616)))\n",
    "\n",
    "    # This starts and runs the streaming of the camera\n",
    "    server = stack.enter_context(StreamingServer(camera))  \n",
    "\n",
    "    print (\"Loading model - hold on ..\")\n",
    "    \n",
    "    # Tune these\n",
    "    speed = 0.2\n",
    "    tolerance = 0.2\n",
    "    duration = 0.2\n",
    "    \n",
    "    # Do inference on VisionBonnet\n",
    "    with CameraInference(face_detection.model()) as inference:\n",
    "        try:   \n",
    "            for result in inference.run():\n",
    "                leds.update(Leds.rgb_on(Color.RED))\n",
    "                faces = face_detection.get_faces(result)\n",
    "                \n",
    "                # This sends the overlay (boxes) to add to the camera stream\n",
    "                server.send_overlay(svg_overlay(faces, (result.width, result.height)))\n",
    "\n",
    "                # If we gor a face, check the offset and move\n",
    "                if len(faces) >= 1:\n",
    "                    clear_output(wait=True)\n",
    "                    offset = pointFace(faces[0].bounding_box, result.width)\n",
    "                    \n",
    "                    if(offset < -tolerance):\n",
    "                        bot.right(speed=speed, duration=duration)\n",
    "                    elif (offset > tolerance):\n",
    "                        bot.left(speed=speed, duration=duration)\n",
    "                    \n",
    "                    leds.update(Leds.rgb_on(Color.GREEN))\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted ..\")\n",
    "            \n",
    "    leds.update(Leds.rgb_off())\n",
    "    \n",
    "    # Servo back to the middle upon ending\n",
    "    bot.stop()\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1edd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
